<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>Home Page</title><meta name="author" content="Guangxin Wang"><link rel="shortcut icon" href="/img/favicon.png"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.13.0/css/all.min.css"><meta name="generator" content="Hexo 7.1.1"></head><body><header id="page_header"><div class="header_wrap"><div id="blog_name"><a class="blog_title" id="site-name" href="/">Home Page</a></div><button class="menus_icon"><div class="navicon"></div></button><ul class="menus_items"><li class="menus_item"><a class="site-page" href="/publications"> Publications</a></li><li class="menus_item"><a class="site-page" href="/about"> About</a></li><li class="menus_item"><a class="site-page" href="/sites"> Sites</a></li></ul></div></header><main id="page_main"><div class="side-card sticky"><div class="card-wrap" itemscope itemtype="http://schema.org/Person"><div class="author-avatar"><img class="avatar-img" src="/img/avatar_rmbg.png" onerror="this.onerror=null;this.src='/img/profile.png'" alt="avatar"></div><div class="author-discrip"><h3>Guangxin Wang</h3><p class="author-bio">Embedded System Engineer</p></div><div class="author-links"><button class="btn m-social-links">Links</button><ul class="social-icons"><li><a class="social-icon" href="https://wgxls.site" target="_blank"><i class="fa fa-home" aria-hidden="true"></i></a></li><li><a class="social-icon" href="https://github.com/WANG-Guangxin" target="_blank"><i class="fab fa-github" aria-hidden="true"></i></a></li><li><a class="social-icon" href="tencent://AddContact/?fromId=50&amp;fromSubId=1&amp;subcmd=all&amp;uin=2401881997" target="_blank"><i class="fab fa-qq" aria-hidden="true"></i></a></li><li><a class="social-icon" href="mailto:wgxls@foxmail.com" target="_blank"><i class="fas fa-envelope" aria-hidden="true"></i></a></li></ul></div></div></div><div class="page" itemscope itemtype="http://schema.org/CreativeWork"><h1 class="page-title">Publications</h1><article><h3 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a>Paper</h3><h4 id="A-natural-based-fusion-strategy-for-underwater-image-enhancement"><a href="#A-natural-based-fusion-strategy-for-underwater-image-enhancement" class="headerlink" title="A natural-based fusion strategy for underwater image enhancement"></a>A natural-based fusion strategy for underwater image enhancement</h4><p><a target="_blank" rel="noopener" href="https://doi.org/10.1007/s11042-022-12267-7">https://doi.org/10.1007/s11042-022-12267-7</a></p>
<p>Multimedia Tools and Applications</p>
<p>Cite:</p>
<blockquote>
<p>@Article{Yan2022,<br>author&#x3D;{Yan, Xiaohong<br>and Wang, Guangxin<br>and Jiang, Guangqi<br>and Wang, Yafei<br>and Mi, Zetian<br>and Fu, Xianping},<br>title&#x3D;{A natural-based fusion strategy for underwater image enhancement},<br>journal&#x3D;{Multimedia Tools and Applications},<br>year&#x3D;{2022},<br>month&#x3D;{Sep},<br>day&#x3D;{01},<br>volume&#x3D;{81},<br>number&#x3D;{21},<br>pages&#x3D;{30051-30068},<br>abstract&#x3D;{Underwater images generally are characterized by color cast and low contrast due to selective absorption and light scattering in water medium. Such degraded images reveal some limitations when used for further analysis. To overcome underwater image degradation, various enhancement techniques are developed. Especially, the fusion-based methods have made remarkable success in this filed. However, there are still some defects in the fusion of input images and weight maps, which cause their results to be unnatural. In this paper, we propose a novel and effective natural-based fusion method for underwater image enhancement that applies several image processing algorithms. First, we design an adaptive underwater image white balance method motivated by our statistical prior to mitigate the impact of color deviation of underwater scenes. We then derive two inputs that represent local detail-improved and global contrast-enhanced versions of the color corrected image. Instead of explicitly estimating weight map, like most existing algorithms, we propose a naturalness-preserving weight map estimation (NP-WME) method, which models the weight map estimation as an optimization problem. Particle swarm optimization (PSO) is used to solve it. Benefiting a proper weighting, the proposed method can achieve a trade-off between detail enhancement and contrast improvement, resulting a natural appearance of the fused image. Through this synthesis, we merge the advantages of different algorithms to obtain the output image. Experimental results show that the proposed method outperforms the several related methods based on quantitative and qualitative evaluations.},<br>issn&#x3D;{1573-7721},<br>doi&#x3D;{10.1007&#x2F;s11042-022-12267-7},<br>url&#x3D;{<a target="_blank" rel="noopener" href="https://doi.org/10.1007/s11042-022-12267-7%7D">https://doi.org/10.1007/s11042-022-12267-7}</a><br>}</p>
</blockquote>
<h4 id="A-novel-biologically-inspired-method-for-underwater-image-enhancement"><a href="#A-novel-biologically-inspired-method-for-underwater-image-enhancement" class="headerlink" title="A novel biologically-inspired method for underwater image enhancement"></a>A novel biologically-inspired method for underwater image enhancement</h4><p><a target="_blank" rel="noopener" href="https://doi.org/10.1016/j.image.2022.116670">https://doi.org/10.1016/j.image.2022.116670</a></p>
<p>Signal Processing: Image Communication</p>
<p>Cite:</p>
<blockquote>
<p>@article{YAN2022116670,<br>title &#x3D; {A novel biologically-inspired method for underwater image enhancement},<br>journal &#x3D; {Signal Processing: Image Communication},<br>volume &#x3D; {104},<br>pages &#x3D; {116670},<br>year &#x3D; {2022},<br>issn &#x3D; {0923-5965},<br>doi &#x3D; {<a target="_blank" rel="noopener" href="https://doi.org/10.1016/j.image.2022.116670%7D">https://doi.org/10.1016/j.image.2022.116670}</a>,<br>url &#x3D; {<a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0923596522000248%7D">https://www.sciencedirect.com/science/article/pii/S0923596522000248}</a>,<br>author &#x3D; {Xiaohong Yan and Guangxin Wang and Guangyuan Wang and Yafei Wang and Xianping Fu},<br>keywords &#x3D; {Underwater image, Biological vision, Color constancy, Luminance adaptation},<br>abstract &#x3D; {Underwater images are usually characterized by color distortion, blurry, and severe noise, because light is severely scattered and absorbed when traveling in the water. In this paper, we propose a novel method motivated by the astonishing capability of the biological vision to address the low visibility of the real-world underwater images. Firstly, we simply imitate the color constancy mechanism in photoreceptors and horizontal cells (HCs) to correct the color distortion. In particular, HCs modulation provides a global color correction with gain control, in which light wavelength-dependent absorption is taken into account. Then, to solve the problems of blurry and noise, we introduce a straightforward and effective two-pathway dehazing method. The core idea is to decompose the color corrected image into structure-pathway and texture-pathway, corresponding to the Magnocellular (M-) and Parvocellular (P-) pathway in the early visual system. In the structure-pathway, we design an innovative biological normalization model to adjust the dynamic range of luminance by integrating the bright and dark regions. By using this approach, the proposed method leads to significant improvement in the contrast degradation of underwater images. Additionally, the detail preservation and noise suppression are implemented on the textural information. Finally, we merge the outputs of structure and texture pathways to reconstruct the enhanced underwater image. Both qualitative and quantitative evaluations show that the proposed biologically-inspired method achieves better visual quality, when compared with several related methods.}<br>}</p>
</blockquote>
<h3 id="Patent"><a href="#Patent" class="headerlink" title="Patent"></a>Patent</h3><h4 id="Underwater-Image-Enhancement-Method-Based-on-Contrast-Perception-Loss"><a href="#Underwater-Image-Enhancement-Method-Based-on-Contrast-Perception-Loss" class="headerlink" title="Underwater Image Enhancement Method Based on Contrast Perception Loss"></a>Underwater Image Enhancement Method Based on Contrast Perception Loss</h4><p><a target="_blank" rel="noopener" href="https://kns.cnki.net/kcms2/article/abstract?v=eoCTaIZmBONhPM4L1JEn4QMh6JvGVb7InF89IykMmLzvZSBK86mVG-GuL-2eoF4yN3gCttr-UptZ7eV4JoWiIS83MjEaXwbtypAOisB_vI-pjxwpfSHVf-4uegffKNx3j9j_yibN9RA=&uniplatform=NZKPT&language=CHS">https://kns.cnki.net/kcms2/article/abstract?v=eoCTaIZmBONhPM4L1JEn4QMh6JvGVb7InF89IykMmLzvZSBK86mVG-GuL-2eoF4yN3gCttr-UptZ7eV4JoWiIS83MjEaXwbtypAOisB_vI-pjxwpfSHVf-4uegffKNx3j9j_yibN9RA=&amp;uniplatform=NZKPT&amp;language=CHS</a></p>
<p>Cite:</p>
<blockquote>
<p>@manual{CN116402721A,<br>author &#x3D; {  付先平 and     曹楠 and     汪广鑫 and     闫小红 and 王亚飞},<br>title &#x3D; {基于对比感知损失的水下图像增强方法},<br>edition &#x3D; {CN116402721A},<br>year &#x3D; {2023},<br>pages &#x3D; {18},<br>address &#x3D; {116026 辽宁省大连市高新园区凌海路1号}<br>}  </p>
</blockquote>
</article></div></main><div class="nav-wrap"><div class="nav"><button class="site-nav"><div class="navicon"></div></button><ul class="nav_items"><li class="nav_item"><a class="nav-page" href="/publications"> Publications</a></li><li class="nav_item"><a class="nav-page" href="/about"> About</a></li><li class="nav_item"><a class="nav-page" href="/sites"> Sites</a></li></ul></div><div class="cd-top"><i class="fa fa-arrow-up" aria-hidden="true"></i></div></div><footer id="page_footer"><div class="footer_wrap"><div class="copyright">&copy;2020 - 2025 by Guangxin Wang</div><div class="theme-info">Powered by <a target="_blank" href="https://hexo.io" rel="nofollow noopener">Hexo</a> & <a target="_blank" href="https://github.com/PhosphorW/hexo-theme-academia" rel="nofollow noopener">Academia Theme</a></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery-pjax@latest/jquery.pjax.min.js"></script><script src="/js/main.js"></script></body></html>