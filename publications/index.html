<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>Home Page</title><meta name="author" content="Guangxin Wang"><link rel="shortcut icon" href="/img/favicon.png"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.13.0/css/all.min.css"><meta name="generator" content="Hexo 7.1.1"></head><body><header id="page_header"><div class="header_wrap"><div id="blog_name"><a class="blog_title" id="site-name" href="/">Home Page</a></div><button class="menus_icon"><div class="navicon"></div></button><ul class="menus_items"><li class="menus_item"><a class="site-page" href="/publications"> Publications</a></li><li class="menus_item"><a class="site-page" href="/about"> About</a></li><li class="menus_item"><a class="site-page" href="/sites"> Sites</a></li></ul></div></header><main id="page_main"><div class="side-card sticky"><div class="card-wrap" itemscope itemtype="http://schema.org/Person"><div class="author-avatar"><img class="avatar-img" src="/img/avatar_rmbg.png" onerror="this.onerror=null;this.src='/img/profile.png'" alt="avatar"></div><div class="author-discrip"><h3>Guangxin Wang</h3><p class="author-bio">Embedded System Engineer</p></div><div class="author-links"><button class="btn m-social-links">Links</button><ul class="social-icons"><li><a class="social-icon" href="https://wgxls.site" target="_blank"><i class="fa fa-home" aria-hidden="true"></i></a></li><li><a class="social-icon" href="https://github.com/WANG-Guangxin" target="_blank"><i class="fab fa-github" aria-hidden="true"></i></a></li><li><a class="social-icon" href="tencent://AddContact/?fromId=50&amp;fromSubId=1&amp;subcmd=all&amp;uin=2401881997" target="_blank"><i class="fab fa-qq" aria-hidden="true"></i></a></li><li><a class="social-icon" href="mailto:wgxls@foxmail.com" target="_blank"><i class="fas fa-envelope" aria-hidden="true"></i></a></li></ul></div></div></div><div class="page" itemscope itemtype="http://schema.org/CreativeWork"><h1 class="page-title">Publications</h1><article><h3 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a>Paper</h3><h4 id="A-natural-based-fusion-strategy-for-underwater-image-enhancement"><a href="#A-natural-based-fusion-strategy-for-underwater-image-enhancement" class="headerlink" title="A natural-based fusion strategy for underwater image enhancement"></a>A natural-based fusion strategy for underwater image enhancement</h4><p><a target="_blank" rel="noopener" href="https://doi.org/10.1007/s11042-022-12267-7">https://doi.org/10.1007/s11042-022-12267-7</a></p>
<p>Multimedia Tools and Applications</p>
<p>Cite:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">@Article&#123;Yan2022,</span><br><span class="line">author=&#123;Yan, Xiaohong</span><br><span class="line">and Wang, Guangxin</span><br><span class="line">and Jiang, Guangqi</span><br><span class="line">and Wang, Yafei</span><br><span class="line">and Mi, Zetian</span><br><span class="line">and Fu, Xianping&#125;,</span><br><span class="line">title=&#123;A natural-based fusion strategy <span class="keyword">for</span> underwater image enhancement&#125;,</span><br><span class="line">journal=&#123;Multimedia Tools and Applications&#125;,</span><br><span class="line">year=&#123;2022&#125;,</span><br><span class="line">month=&#123;Sep&#125;,</span><br><span class="line">day=&#123;01&#125;,</span><br><span class="line">volume=&#123;81&#125;,</span><br><span class="line">number=&#123;21&#125;,</span><br><span class="line">pages=&#123;30051-30068&#125;,</span><br><span class="line">abstract=&#123;Underwater images generally are characterized by color cast and low contrast due to selective absorption and light scattering <span class="keyword">in</span> water medium. Such degraded images reveal some limitations when used <span class="keyword">for</span> further analysis. To overcome underwater image degradation, various enhancement techniques are developed. Especially, the fusion-based methods have made remarkable success <span class="keyword">in</span> this filed. However, there are still some defects <span class="keyword">in</span> the fusion of input images and weight maps, <span class="built_in">which</span> cause their results to be unnatural. In this paper, we propose a novel and effective natural-based fusion method <span class="keyword">for</span> underwater image enhancement that applies several image processing algorithms. First, we design an adaptive underwater image white balance method motivated by our statistical prior to mitigate the impact of color deviation of underwater scenes. We <span class="keyword">then</span> derive two inputs that represent <span class="built_in">local</span> detail-improved and global contrast-enhanced versions of the color corrected image. Instead of explicitly estimating weight map, like most existing algorithms, we propose a naturalness-preserving weight map estimation (NP-WME) method, <span class="built_in">which</span> models the weight map estimation as an optimization problem. Particle swarm optimization (PSO) is used to solve it. Benefiting a proper weighting, the proposed method can achieve a trade-off between detail enhancement and contrast improvement, resulting a natural appearance of the fused image. Through this synthesis, we merge the advantages of different algorithms to obtain the output image. Experimental results show that the proposed method outperforms the several related methods based on quantitative and qualitative evaluations.&#125;,</span><br><span class="line">issn=&#123;1573-7721&#125;,</span><br><span class="line">doi=&#123;10.1007/s11042-022-12267-7&#125;,</span><br><span class="line">url=&#123;https://doi.org/10.1007/s11042-022-12267-7&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h4 id="A-novel-biologically-inspired-method-for-underwater-image-enhancement"><a href="#A-novel-biologically-inspired-method-for-underwater-image-enhancement" class="headerlink" title="A novel biologically-inspired method for underwater image enhancement"></a>A novel biologically-inspired method for underwater image enhancement</h4><p><a target="_blank" rel="noopener" href="https://doi.org/10.1016/j.image.2022.116670">https://doi.org/10.1016/j.image.2022.116670</a></p>
<p>Signal Processing: Image Communication</p>
<p>Cite:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">@article&#123;YAN2022116670,</span><br><span class="line">title = &#123;A novel biologically-inspired method <span class="keyword">for</span> underwater image enhancement&#125;,</span><br><span class="line">journal = &#123;Signal Processing: Image Communication&#125;,</span><br><span class="line">volume = &#123;104&#125;,</span><br><span class="line">pages = &#123;116670&#125;,</span><br><span class="line">year = &#123;2022&#125;,</span><br><span class="line">issn = &#123;0923-5965&#125;,</span><br><span class="line">doi = &#123;https://doi.org/10.1016/j.image.2022.116670&#125;,</span><br><span class="line">url = &#123;https://www.sciencedirect.com/science/article/pii/S0923596522000248&#125;,</span><br><span class="line">author = &#123;Xiaohong Yan and Guangxin Wang and Guangyuan Wang and Yafei Wang and Xianping Fu&#125;,</span><br><span class="line">keywords = &#123;Underwater image, Biological vision, Color constancy, Luminance adaptation&#125;,</span><br><span class="line">abstract = &#123;Underwater images are usually characterized by color distortion, blurry, and severe noise, because light is severely scattered and absorbed when traveling <span class="keyword">in</span> the water. In this paper, we propose a novel method motivated by the astonishing capability of the biological vision to address the low visibility of the real-world underwater images. Firstly, we simply imitate the color constancy mechanism <span class="keyword">in</span> photoreceptors and horizontal cells (HCs) to correct the color distortion. In particular, HCs modulation provides a global color correction with gain control, <span class="keyword">in</span> <span class="built_in">which</span> light wavelength-dependent absorption is taken into account. Then, to solve the problems of blurry and noise, we introduce a straightforward and effective two-pathway dehazing method. The core idea is to decompose the color corrected image into structure-pathway and texture-pathway, corresponding to the Magnocellular (M-) and Parvocellular (P-) pathway <span class="keyword">in</span> the early visual system. In the structure-pathway, we design an innovative biological normalization model to adjust the dynamic range of luminance by integrating the bright and dark regions. By using this approach, the proposed method leads to significant improvement <span class="keyword">in</span> the contrast degradation of underwater images. Additionally, the detail preservation and noise suppression are implemented on the textural information. Finally, we merge the outputs of structure and texture pathways to reconstruct the enhanced underwater image. Both qualitative and quantitative evaluations show that the proposed biologically-inspired method achieves better visual quality, when compared with several related methods.&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Patent"><a href="#Patent" class="headerlink" title="Patent"></a>Patent</h3><h4 id="Underwater-Image-Enhancement-Method-Based-on-Contrast-Perception-Loss"><a href="#Underwater-Image-Enhancement-Method-Based-on-Contrast-Perception-Loss" class="headerlink" title="Underwater Image Enhancement Method Based on Contrast Perception Loss"></a>Underwater Image Enhancement Method Based on Contrast Perception Loss</h4><p><a target="_blank" rel="noopener" href="https://kns.cnki.net/kcms2/article/abstract?v=eoCTaIZmBONhPM4L1JEn4QMh6JvGVb7InF89IykMmLzvZSBK86mVG-GuL-2eoF4yN3gCttr-UptZ7eV4JoWiIS83MjEaXwbtypAOisB_vI-pjxwpfSHVf-4uegffKNx3j9j_yibN9RA=&uniplatform=NZKPT&language=CHS">https://kns.cnki.net/kcms2/article/abstract?v=eoCTaIZmBONhPM4L1JEn4QMh6JvGVb7InF89IykMmLzvZSBK86mVG-GuL-2eoF4yN3gCttr-UptZ7eV4JoWiIS83MjEaXwbtypAOisB_vI-pjxwpfSHVf-4uegffKNx3j9j_yibN9RA=&amp;uniplatform=NZKPT&amp;language=CHS</a></p>
<p>Cite:</p>
<pre><code class="bash">
@manual&#123;CN116402721A,
author = &#123;  付先平 and     曹楠 and     汪广鑫 and     闫小红 and 王亚飞&#125;,
 title = &#123;基于对比感知损失的水下图像增强方法&#125;,
edition = &#123;CN116402721A&#125;,
year = &#123;2023&#125;,
pages = &#123;18&#125;,
address = &#123;116026 辽宁省大连市高新园区凌海路1号&#125;
&#125;  
</code></pre>
</article></div></main><div class="nav-wrap"><div class="nav"><button class="site-nav"><div class="navicon"></div></button><ul class="nav_items"><li class="nav_item"><a class="nav-page" href="/publications"> Publications</a></li><li class="nav_item"><a class="nav-page" href="/about"> About</a></li><li class="nav_item"><a class="nav-page" href="/sites"> Sites</a></li></ul></div><div class="cd-top"><i class="fa fa-arrow-up" aria-hidden="true"></i></div></div><footer id="page_footer"><div class="footer_wrap"><div class="copyright">&copy;2020 - 2024 by Guangxin Wang</div><div class="theme-info">Powered by <a target="_blank" href="https://hexo.io" rel="nofollow noopener">Hexo</a> & <a target="_blank" href="https://github.com/PhosphorW/hexo-theme-academia" rel="nofollow noopener">Academia Theme</a></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery-pjax@latest/jquery.pjax.min.js"></script><script src="/js/main.js"></script></body></html>